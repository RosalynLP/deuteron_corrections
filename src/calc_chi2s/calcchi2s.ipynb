{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes: \n",
    "# 1. You need to put the covariance matrix csv files in this folder\n",
    "# 2. Some of the cells can be a bit computationally expensive, hence I save the output \n",
    "#    as pickles to be easily reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validphys.calcutils import calc_chi2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from validphys.core import ExperimentSpec, FKTableSpec\n",
    "from validphys import results\n",
    "from validphys.loader import Loader\n",
    "from validphys.api import API\n",
    "import scipy.linalg as la\n",
    "\n",
    "# Reading in deuteron covmats\n",
    "it0 = pd.read_csv(\n",
    "    \"covmatrix_global_proton.csv\",\n",
    "    dtype={\"user_id\": float},\n",
    "    index_col=[0,1,2], header=[0,1,2]\n",
    ")\n",
    "\n",
    "it1dw = pd.read_csv(\n",
    "    \"covmatrix_ite.csv\",\n",
    "    dtype={\"user_id\": float},\n",
    "    index_col=[0,1,2], header=[0,1,2]\n",
    ")\n",
    "\n",
    "it1shift = pd.read_csv(\n",
    "    \"covmatrix_shift_ite_1.csv\",\n",
    "    dtype={\"user_id\": float},\n",
    "    index_col=[0,1,2], header=[0,1,2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming experiment DYE886R -> DYE886 to correct covariance matrix.\n",
    "# Commented out because it was changed manually in the csv file.\n",
    "#tups = []\n",
    "# for tup in it0.index:\n",
    "#     if tup[0] == \"DYE886R\":\n",
    "#         newtup = (\"DYE886\", tup[1], tup[2])\n",
    "#     else: newtup = tup\n",
    "#     tups.append(newtup)\n",
    "# newindex = pd.MultiIndex.from_tuples(tups, names=(\"experiment\", \"dataset\", \"id\"))\n",
    "\n",
    "# Relabelling dataframes because otherwise column index is a string rather than an int and this causes problems\n",
    "# down the line\n",
    "it0 = pd.DataFrame(it0.values, index=it0.index, columns=it0.index)\n",
    "it1dw = pd.DataFrame(it1dw.values, index=it1dw.index, columns=it1dw.index)\n",
    "it1shift = pd.DataFrame(it1shift.values, index=it1shift.index, columns=it1shift.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset index so we can wrap everything in a dataframe to prevent misalignment\n",
    "dsindex_bl = API.experiments_index(experiments={\"from_\": \"fit\"},\n",
    "                                   fit=\"200609-ern-001\",\n",
    "                                   theoryid=53,\n",
    "                                   use_cuts=\"fromfit\",\n",
    "                                   pdf={\"from_\": \"fit\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting D and T to calculate diffs\n",
    "datth_bl = API.experiments_results(experiments={\"from_\": \"fit\"},\n",
    "                                   fit=\"200609-ern-001\",\n",
    "                                   theoryid=53,\n",
    "                                   use_cuts=\"fromfit\",\n",
    "                                   pdf={\"from_\": \"fit\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for experiment in datth_bl:\n",
    "    diffs.append(experiment[0].central_value - experiment[1].central_value)\n",
    "diffs_bl = pd.DataFrame([item for sublist in diffs for item in sublist], index=dsindex_bl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datth_it0 = API.experiments_results(experiments={\"from_\": \"fit\"},\n",
    "                            fit=\"NNPDF31_nnlo_as_0118_global_deut\",\n",
    "                            theoryid=53,\n",
    "                            use_cuts=\"fromfit\",\n",
    "                            pdf={\"from_\": \"fit\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for experiment in datth_it0:\n",
    "    diffs.append(experiment[0].central_value - experiment[1].central_value)\n",
    "diffs_it0 =pd.DataFrame([item for sublist in diffs for item in sublist], index=dsindex_bl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datth_it1dw = API.experiments_results(experiments={\"from_\": \"fit\"},\n",
    "                                     fit=\"NNPDF31_nnlo_as_0118_global_deut_ite\",\n",
    "                                     theoryid=53,\n",
    "                                     use_cuts=\"fromfit\",\n",
    "                                     pdf={\"from_\": \"fit\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for experiment in datth_it1dw:\n",
    "    diffs.append(experiment[0].central_value - experiment[1].central_value)\n",
    "diffs_it1dw =pd.DataFrame([item for sublist in diffs for item in sublist], index=dsindex_bl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datth_it1shifted = API.experiments_results(experiments={\"from_\": \"fit\"},\n",
    "                                     fit=\"NNPDF31_nnlo_as_0118_global_deut_ite_shift\",\n",
    "                                     theoryid=53,\n",
    "                                     use_cuts=\"fromfit\",\n",
    "                                     pdf={\"from_\": \"fit\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for experiment in datth_it1shifted:\n",
    "    diffs.append(experiment[0].central_value - experiment[1].central_value)\n",
    "diffs_it1shifted =pd.DataFrame([item for sublist in diffs for item in sublist], index=dsindex_bl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diffs_bl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cfa241326ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pickling items for easy reloading - from now on can skip cells 5-13\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdiffs_bl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"diffs_bl.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdiffs_it0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"diffs_it0.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdiffs_it1dw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"diffs_it1dw.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'diffs_bl' is not defined"
     ]
    }
   ],
   "source": [
    "# Pickling items for easy reloading - from now on can skip cells 5-13\n",
    "import pickle\n",
    "pickle.dump( diffs_bl, open( \"diffs_bl.p\", \"wb\" ) )\n",
    "pickle.dump( diffs_it0, open( \"diffs_it0.p\", \"wb\" ) )\n",
    "pickle.dump( diffs_it1dw, open( \"diffs_it1dw.p\", \"wb\" ) )\n",
    "pickle.dump( diffs_it1shifted, open( \"diffs_it1shifted.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading pickles\n",
    "diffs_bl = pd.read_pickle(\"diffs_bl.p\")\n",
    "diffs_it0 = pd.read_pickle(\"diffs_it0.p\")\n",
    "diffs_it1dw = pd.read_pickle(\"diffs_it1dw.p\")\n",
    "diffs_it1shifted = pd.read_pickle(\"diffs_it1shifted.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diffs_it0 = pd.DataFrame(diffs_it0.values, index=C_orig.index)\n",
    "#diffs_bl = pd.DataFrame(diffs_bl.values, index=newindex)\n",
    "#diffs_it1dw = pd.DataFrame(diffs_it1dw.values, index=newindex)\n",
    "#diffs_it1shifted = pd.DataFrame(diffs_it1shifted.values, index=newindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s1303034/miniconda3/envs/nnpdf-dev/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3417: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/s1303034/miniconda3/envs/nnpdf-dev/lib/python3.7/site-packages/reportengine/api.py:43: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  return self.__call__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Loading original covmat (experimental)\n",
    "C_orig = API.experiments_covmat( experiments={\"from_\": \"fit\"},\n",
    "                                   fit =\"200609-ern-001\",\n",
    "                                   theoryid=53,\n",
    "                                   use_cuts=\"fromfit\",\n",
    "                                   pdf={\"from_\": \"fit\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets in orig covmat and in deuteron fit\n",
    "dslist = list(dict.fromkeys([tup[1] for tup in C_orig.index]))\n",
    "dslist_small = list(dict.fromkeys([tup[1] for tup in it0.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extend the dimensions of a small covmat to that of a big covmat, filling in the empty entries with 0s\n",
    "def extend_covmat(dslist, bigcovmat, smallcovmat):\n",
    "    # Make dimensions match those of exp covmat. First make empty df of\n",
    "    # exp covmat dimensions\n",
    "    empty_df = pd.DataFrame(0, index=C_orig.index, columns=C_orig.index)\n",
    "    covmats = []\n",
    "    for ds1 in dslist:\n",
    "        for ds2 in dslist:\n",
    "            if (ds1 in smallcovmat.index.unique(level=1)) and (ds2 in smallcovmat.index.unique(level=1)):\n",
    "                # If both datasets are in the small covmat, take the relevant ds covmat out the small covmat\n",
    "                covmat = smallcovmat.xs(ds1,level=1, drop_level=False).T.xs(ds2, level=1, drop_level=False).T\n",
    "            else:\n",
    "                # If not, make a ds covmat of 0s of the relevant dimensions \n",
    "                covmat = empty_df.xs(ds1,level=1, drop_level=False).T.xs(ds2, level=1, drop_level=False).T\n",
    "            covmat.reset_index()\n",
    "            # covmats is a list of the ds covmats in order\n",
    "            covmats.append(covmat)\n",
    "    # Chunks is a list of lists of covmats, one list of covmats for each dataset\n",
    "    chunks = []\n",
    "    for x in range(0, len(covmats), len(dslist)):\n",
    "        chunk = covmats[x:x+len(dslist)]\n",
    "        chunks.append(chunk)\n",
    "    # Concatenate each chunk into a strip so we have N_dataset strips of covmat\n",
    "    strips = []\n",
    "    i=0\n",
    "    for chunk in chunks:\n",
    "        i=i+1\n",
    "        strip = pd.concat(chunk, axis=1)\n",
    "        strips.append(strip.T)\n",
    "    strips.reverse()\n",
    "    # Stack the strips to construct the full covmat\n",
    "    full_df = pd.concat(strips, axis=1)\n",
    "    full_df = full_df.reindex(bigcovmat.index)\n",
    "    full_df = ((full_df.T).reindex(bigcovmat.index)).T\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending the deuteron covmats to match size of C_orig\n",
    "it0total = extend_covmat(dslist, C_orig, it0)\n",
    "it1dwtotal = extend_covmat(dslist, C_orig, it1dw)\n",
    "it1shifttotal = extend_covmat(dslist, C_orig, it1shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.17868292])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating total chi2s\n",
    "calc_chi2(la.cholesky(C_orig, lower=True), diffs_bl)/len(diffs_bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.16000901])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_chi2(la.cholesky(it0total+C_orig, lower=True), diffs_it0)/len(diffs_it0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.15869441])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_chi2(la.cholesky(it1dwtotal+C_orig, lower=True), diffs_it1dw)/len(diffs_it1dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.16601756])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_chi2(la.cholesky(it1shifttotal+C_orig, lower=True), diffs_it1shifted)/len(diffs_it1shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return chi2s by dataset\n",
    "def chi2s_by_dataset(covmat, diffs):\n",
    "    chi2s = []\n",
    "    for dataset in dslist:\n",
    "        dscovmat = covmat.xs(dataset,level=1, drop_level=False).T.xs(dataset, level=1, drop_level=False).T\n",
    "        dsdiffs = diffs.xs(dataset, level=1, drop_level=False)\n",
    "        chi2 = calc_chi2(la.cholesky(dscovmat, lower=True), dsdiffs)/len(dsdiffs)\n",
    "        chi2s.append((dataset, chi2[0]))\n",
    "    chi2table = pd.DataFrame(chi2s, columns=[\"dataset\", \"chi2\"])\n",
    "    return chi2table\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chi2s_by_dataset(C_orig+it1dwtotal, diffs_it1dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = {\"NMC\":[\"NMC\", \"NMCPD\"], \"SLAC\": [\"SLACP\", \"SLACD\"], \"BCDMS\":[\"BCDMSP\", \"BCDMSD\"]}\n",
    "explist = list(dict.fromkeys([tup[0] for tup in C_orig.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2s_by_custom_grouping(covmat, diffs, exceptions):\n",
    "    chi2s = []\n",
    "    index = []\n",
    "    for exp in explist:\n",
    "        if exp in exceptions:\n",
    "            datasets = exceptions[exp]\n",
    "            for dataset in datasets:\n",
    "                dscovmat = covmat.xs(dataset,level=1, drop_level=False).T.xs(dataset, level=1, drop_level=False).T\n",
    "                dsdiffs = diffs.xs(dataset, level=1, drop_level=False)\n",
    "                chi2 = calc_chi2(la.cholesky(dscovmat, lower=True), dsdiffs)/len(dsdiffs)\n",
    "                index.append(dataset)\n",
    "                chi2s.append(chi2[0])\n",
    "        else:\n",
    "            dscovmat = covmat.xs(exp,level=0, drop_level=False).T.xs(exp, level=0, drop_level=False).T\n",
    "            dsdiffs = diffs.xs(exp, level=0, drop_level=False)\n",
    "            chi2 = calc_chi2(la.cholesky(dscovmat, lower=True), dsdiffs)/len(dsdiffs)\n",
    "            index.append(exp)\n",
    "            chi2s.append(chi2[0])\n",
    "    chi2table = pd.DataFrame(chi2s, index=index, columns=[r\"$\\chi^2$\"])\n",
    "    return chi2table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "blchi2 = chi2s_by_custom_grouping(C_orig, diffs_bl, exceptions)\n",
    "print(blchi2.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it0chi2 = chi2s_by_custom_grouping(C_orig, diffs_bl, exceptions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
